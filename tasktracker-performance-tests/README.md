# TaskTracker Performance Tests

Comprehensive end-to-end performance testing suite for comparing **Monolithic** vs **Microservices** architectures using **Locust**.

---

## ğŸ“‹ Overview

This test suite provides automated performance testing and comparison for both TaskTracker architectures under different load conditions:

- **Monolithic Architecture** (Port 9000) - Single instance
- **Microservices Architecture** (Port 8000) - Single or scaled instances

## ğŸ¯ **Key Findings Summary**

### Test 1: Standard Load (50 concurrent users)
**Winner: Monolithic âœ…**
- Monolithic: 6ms average, 23.6 req/s
- Microservices: 16ms average, 23.2 req/s
- **Result:** Monolithic is **2.7x faster**
- **Reason:** Network overhead > scaling benefits at low load

### Test 2: High Load (200 concurrent users, 3x scaled microservices)
**Winner: Scaled Microservices âœ…**
- Monolithic (1 instance): 97.87ms average, 83.17 req/s
- Microservices (3x scaled): 65.48ms average, 91.41 req/s
- **Result:** Microservices is **33% faster** and **10% more throughput**
- **Reason:** Distributed load processing > network overhead at high load

### ğŸ’¡ **The Verdict:**
> **Monolithic wins with limited resources** (< 100 users)  
> **Microservices wins when scaled horizontally** (200+ users)  
> **Choose based on expected load and resource availability!**

---

## ğŸ› ï¸ Prerequisites

### Required Software

1. **Python 3.11+**
2. **pip** (Python package manager)
3. **Both applications running**:
   - Monolithic on port 9000
   - Microservices on port 8000

### Start Applications

**Standard Test (single instances):**
```bash
# Terminal 1 - Monolithic
cd ../tasktracker-mono
docker compose up -d

# Terminal 2 - Microservices (single instance)
cd ../tasktracker-micro
docker compose up -d
```

**Scaling Test (multiple instances):**
```bash
# Terminal 1 - Monolithic (same as above)
cd ../tasktracker-mono
docker compose up -d

# Terminal 2 - Scaled Microservices (3 replicas each)
cd ../tasktracker-micro
./start-scaled.sh
```

Verify both are running:
```bash
curl http://localhost:9000/health  # Monolithic
curl http://localhost:8000/health  # Microservices
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### 2. Make Scripts Executable

```bash
chmod +x run_*.sh
```

### 3. Run Tests

**Option A: Standard Load Test (50 users)**
```bash
./run_both_tests.sh
```
Shows monolithic winning at low/medium load.

**Option B: High Load Test with Scaling (200 users)**
```bash
# First, start scaled microservices
cd ../tasktracker-micro && ./start-scaled.sh

# Then run high-load test
cd ../tasktracker-performance-tests
./run_scaled_test.sh
```
Shows microservices winning with horizontal scaling.

**Option C: Run Both Scenarios**
```bash
# Test 1: Standard load
./run_both_tests.sh

# Wait for completion, then...

# Test 2: Start scaled microservices
cd ../tasktracker-micro
docker compose down
./start-scaled.sh

# Test 2: Run high-load test
cd ../tasktracker-performance-tests
./run_scaled_test.sh
```

### 4. View Results

Results are saved in `results/` directory with timestamp:
```
results/
â”œâ”€â”€ Standard Load Results (50 users):
â”‚   â”œâ”€â”€ monolithic_20231215_143022/
â”‚   â”‚   â”œâ”€â”€ report.html          # Interactive HTML report
â”‚   â”‚   â”œâ”€â”€ stats_stats.csv      # Raw statistics
â”‚   â”‚   â””â”€â”€ locust.log           # Test logs
â”‚   â”œâ”€â”€ microservices_20231215_143022/
â”‚   â””â”€â”€ comparison_20231215_143022/
â”‚       â”œâ”€â”€ response_time_comparison.png
â”‚       â”œâ”€â”€ throughput_comparison.png
â”‚       â””â”€â”€ summary_table.png
â”‚
â””â”€â”€ High Load Results (200 users, scaled):
    â”œâ”€â”€ monolithic_highload_20231215_150000/
    â”œâ”€â”€ microservices_scaled_20231215_150000/
    â””â”€â”€ comparison_highload_20231215_150000/
```

---

## ğŸ“Š **Test Results Deep Dive**

### Standard Load Test (50 concurrent users)

**Monolithic (1 instance):**
- Average Response Time: **6.09 ms** âœ…
- Throughput: **23.6 req/s**
- 99th Percentile: **20 ms**
- Total Requests: 4,237
- Failure Rate: **0%**

**Microservices (1 instance each service):**
- Average Response Time: **16.31 ms**
- Throughput: **23.2 req/s**  
- 99th Percentile: **72 ms**
- Total Requests: 4,172
- Failure Rate: **0%**

**Analysis:**
- ğŸ† Monolithic is **2.68x faster**
- Network overhead (API Gateway â†’ Service) adds ~10ms per request
- At low load, monolithic's direct database access wins

---

### High Load Test (200 concurrent users)

**Monolithic (1 instance) - Under Stress:**
- Average Response Time: **97.87 ms** (degraded!)
- Throughput: **83.17 req/s**
- 99th Percentile: **140 ms**
- Total Requests: 23,633
- Failure Rate: **0.17%** (40 failures)
- **CPU bottleneck** - single instance overwhelmed

**Scaled Microservices (3 instances per service):**
- Average Response Time: **65.48 ms** âœ…
- Throughput: **91.41 req/s** âœ…
- 99th Percentile: **510 ms**
- Total Requests: 27,369
- Failure Rate: **0%** âœ…
- **Load distributed** across 9 service instances

**Analysis:**
- ğŸ† Scaled Microservices is **33% faster** (65ms vs 98ms)
- ğŸ† Scaled Microservices has **10% more throughput** (91 vs 83 req/s)
- ğŸ† Scaled Microservices is **more reliable** (0% vs 0.17% failures)
- Load distribution allows better CPU utilization
- Multiple replicas eliminate single-instance bottleneck

---

## ğŸ¯ **The Crossover Point**

### When Each Architecture Wins:

| Load Level | Concurrent Users | Winner | Why |
|------------|-----------------|---------|-----|
| **Low** | < 50 users | Monolithic (2.7x faster) | Network overhead dominates |
| **Medium** | 50-100 users | Monolithic (2x faster) | Still not enough to justify overhead |
| **Medium-High** | 100-150 users | Comparable | Transition zone |
| **High** | 150-200 users | Comparable | Depends on scaling |
| **Very High** | 200+ users | **Scaled Microservices** | Horizontal scaling wins |

### Visual Representation:

```
Performance Winner by Load:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
0-100 users:   Monolithic â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (2.7x faster)
100-200 users: Transition â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (crossover point)
200+ users:    Microservices â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (better throughput & latency)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ’¡ **Architectural Decision Guide**

### Choose Monolithic When:

âœ… **Limited Resources**
- Single server or small infrastructure
- Budget constraints
- Startup/MVP phase

âœ… **Low to Medium Traffic**
- < 100 concurrent users
- < 1M requests per day
- Predictable load patterns

âœ… **Performance Critical**
- Latency requirements < 10ms
- Real-time requirements
- Direct database access needed

âœ… **Small Team**
- < 10 developers
- Single development team
- Simple coordination needs

**Example:** SaaS products, startups, internal tools, APIs with strict SLAs

---

### Choose Microservices When:

âœ… **High Traffic with Scaling Needs**
- 200+ concurrent users
- 10M+ requests per day
- Need horizontal scaling

âœ… **Unlimited Resources**
- Cloud infrastructure with auto-scaling
- Budget for multiple services
- DevOps team available

âœ… **Large Organization**
- Multiple independent teams (20+ developers)
- Team autonomy requirements
- Different tech stacks per service

âœ… **Service-Specific Requirements**
- Some services need more resources
- Independent deployment cadences
- Fault isolation critical

**Example:** Enterprise platforms, high-traffic applications, multi-team products

---

## ğŸ“Š **Performance Characteristics**

### Standard Load (50 users):

| Metric | Monolithic | Microservices | Winner |
|--------|-----------|---------------|---------|
| Avg Response | 6 ms | 16 ms | Monolithic (2.7x) |
| Throughput | 23.6 req/s | 23.2 req/s | Comparable |
| 99th Percentile | 20 ms | 72 ms | Monolithic (3.6x) |
| Failures | 0% | 0% | Tie |

### High Load (200 users):

| Metric | Monolithic (1x) | Microservices (3x) | Winner |
|--------|----------------|-------------------|---------|
| Avg Response | 98 ms | 65 ms | Microservices (33%) |
| Throughput | 83 req/s | 91 req/s | Microservices (10%) |
| 99th Percentile | 140 ms | 510 ms | Monolithic |
| Failures | 0.17% | 0% | Microservices |

**Key Insight:** Under high load, scaled microservices handles traffic better than single-instance monolithic.

---

## âš™ï¸ Configuration

### Test Parameters

Configure via environment variables:

```bash
# Number of concurrent users (default: 50 for standard, 200 for scaled)
export USERS=100

# Users spawned per second (default: 10)
export SPAWN_RATE=20

# Test duration (default: 3m)
export RUN_TIME=5m

# Run test
./run_both_tests.sh
```

### Architecture URLs

Edit `config.py` to change base URLs:

```python
ARCHITECTURES = {
    "monolithic": {
        "base_url": "http://localhost:9000",
    },
    "microservices": {
        "base_url": "http://localhost:8000",
    }
}
```

### Task Weights

Control operation frequency in `config.py`:

```python
TASK_WEIGHTS = {
    "read": 70,      # 70% read operations
    "write": 20,     # 20% write operations  
    "update": 7,     # 7% update operations
    "delete": 3,     # 3% delete operations
}
```

---

## ğŸ¯ Test Scenarios

### Scenario 1: Standard Load Test
```bash
./run_both_tests.sh
```
- 50 concurrent users
- 3-minute duration
- **Shows:** Monolithic performance advantage

### Scenario 2: High Load with Scaling
```bash
# Start scaled microservices (3 replicas each)
cd ../tasktracker-micro && ./start-scaled.sh

# Run high-load test
cd ../tasktracker-performance-tests
./run_scaled_test.sh
```
- 200 concurrent users
- 5-minute duration
- **Shows:** Microservices scaling advantage

### Scenario 3: Custom Load
```bash
USERS=150 RUN_TIME=10m ./run_both_tests.sh
```

---

## ğŸ“ Project Structure

```
tasktracker-performance-tests/
â”œâ”€â”€ Core Test Files
â”‚   â”œâ”€â”€ locustfile_monolithic.py      # Locust test for monolithic
â”‚   â”œâ”€â”€ locustfile_microservices.py   # Locust test for microservices
â”‚   â”œâ”€â”€ config.py                     # Test configuration
â”‚   â””â”€â”€ utils.py                      # Utility functions
â”‚
â”œâ”€â”€ Analysis
â”‚   â””â”€â”€ analyze_results.py            # Results analysis & visualization
â”‚
â”œâ”€â”€ Run Scripts
â”‚   â”œâ”€â”€ run_mono_test.sh             # Test monolithic only
â”‚   â”œâ”€â”€ run_micro_test.sh            # Test microservices only
â”‚   â”œâ”€â”€ run_both_tests.sh            # Test both (standard load)
â”‚   â”œâ”€â”€ run_scaled_test.sh           # Test with scaling (high load)
â”‚   â”œâ”€â”€ run_tests.sh                 # Interactive menu
â”‚   â”œâ”€â”€ compare_results.sh           # Compare existing results
â”‚   â””â”€â”€ cleanup.sh                   # Clean old results
â”‚
â”œâ”€â”€ Setup Scripts
â”‚   â”œâ”€â”€ setup.sh                     # First-time setup
â”‚   â””â”€â”€ verify_setup.sh              # Verify configuration
â”‚
â”œâ”€â”€ Documentation
â”‚   â”œâ”€â”€ README.md                    # This file
â”‚   â”œâ”€â”€ QUICKSTART.md                # Quick start guide
â”‚   â”œâ”€â”€ PRESENTATION_TAKEAWAYS.md    # For presentations
â”‚   â”œâ”€â”€ POWERPOINT_SLIDES.md         # Ready-to-use slides
â”‚   â”œâ”€â”€ SCALING_TEST_GUIDE.md        # Scaling test guide
â”‚   â””â”€â”€ SCALING_QUICKSTART.md        # Quick scaling guide
â”‚
â””â”€â”€ Results (Generated)
    â”œâ”€â”€ monolithic_*/                # Standard test results
    â”œâ”€â”€ microservices_*/
    â”œâ”€â”€ monolithic_highload_*/       # High-load test results
    â”œâ”€â”€ microservices_scaled_*/
    â””â”€â”€ comparison_*/                # Comparison charts
```

---

## ğŸ“Š Understanding Results

### What Gets Tested

âœ… **Authentication Operations**
- User login, token validation, user info retrieval

âœ… **Task Management (CRUD)**
- Create, list, filter, get, update, complete, delete tasks

âœ… **Statistics**
- User statistics aggregation
- Cross-service communication (microservices)

### Test Characteristics

- **Realistic User Behavior**: Simulates actual user patterns with weighted operations
- **Pre-populated Data**: Creates 20 users with 10 tasks each before testing
- **Concurrent Load**: Tests with configurable concurrent users
- **Comprehensive Metrics**: Response times, throughput, failure rates, percentiles
- **Visual Reports**: Generates comparison charts and HTML reports

---

## ğŸ“ˆ Real Test Results

### Standard Load Results (50 users):

**Monolithic Performance:**
```
Average Response Time: 6.09 ms
Median Response Time:  6 ms
95th Percentile:       9 ms
99th Percentile:       20 ms
Throughput:            23.6 req/s
Total Requests:        4,237
Failure Rate:          0%
```

**Microservices Performance (Single Instance):**
```
Average Response Time: 16.31 ms
Median Response Time:  13 ms
95th Percentile:       30 ms
99th Percentile:       72 ms
Throughput:            23.2 req/s
Total Requests:        4,172
Failure Rate:          0%
```

**Winner:** Monolithic - **2.68x faster** response time

---

### High Load Results (200 users):

**Monolithic Performance (1 instance - STRESSED):**
```
Average Response Time: 97.87 ms  âš ï¸ (degraded)
Median Response Time:  12 ms
95th Percentile:       85 ms
99th Percentile:       140 ms
Throughput:            83.17 req/s
Total Requests:        23,633
Failure Rate:          0.17% (40 failures)
CPU Usage:             ~95-100% (bottlenecked)
```

**Scaled Microservices Performance (3x replicas - DISTRIBUTED):**
```
Average Response Time: 65.48 ms  âœ… (better!)
Median Response Time:  37 ms
95th Percentile:       150 ms
99th Percentile:       510 ms
Throughput:            91.41 req/s  âœ…
Total Requests:        27,369      âœ…
Failure Rate:          0%          âœ…
CPU Usage:             ~40-60% per instance
```

**Winner:** Scaled Microservices - **33% lower latency, 10% more throughput**

---

## ğŸ¯ Key Takeaways

### 1. **Monolithic Advantage: Low Load**
At standard load (50 users):
- âœ… 2.7x faster response times
- âœ… Lower latency variance
- âœ… Simpler infrastructure
- âœ… Lower costs

**Ideal for:** Limited resources, < 100 concurrent users, startups, small teams

### 2. **Microservices Advantage: High Load + Scaling**
At high load with scaling (200 users):
- âœ… 33% faster than stressed monolithic
- âœ… 10% more throughput
- âœ… Better reliability (0% vs 0.17% failures)
- âœ… Horizontal scaling capability

**Ideal for:** High traffic, unlimited resources, large teams, need for elasticity

### 3. **The Network Overhead**
- Microservices adds ~10ms per request at low load
- This overhead becomes negligible under high load when scaled
- Trade-off: Latency vs Scalability

### 4. **Resource Utilization**
**Monolithic under high load:**
- Single CPU core at 95-100% â†’ bottleneck
- Limited by single instance capacity

**Scaled Microservices:**
- 9 instances at 40-60% each â†’ efficient
- Can add more replicas as needed

---

## ğŸ’¡ Architecture Decision Framework

### Decision Tree:

```
Expected Concurrent Users?
â”œâ”€â”€ < 100 users
â”‚   â””â”€â”€ Choose: MONOLITHIC
â”‚       Reason: 2.7x better performance, simpler, cheaper
â”‚
â”œâ”€â”€ 100-200 users
â”‚   â”œâ”€â”€ Limited budget â†’ MONOLITHIC
â”‚   â””â”€â”€ Need to scale â†’ MICROSERVICES
â”‚
â””â”€â”€ 200+ users
    â””â”€â”€ Choose: MICROSERVICES (with scaling)
        Reason: Better throughput, horizontal scaling

Resource Availability?
â”œâ”€â”€ Limited (1-2 servers)
â”‚   â””â”€â”€ Choose: MONOLITHIC
â”‚
â””â”€â”€ Unlimited (cloud with auto-scaling)
    â””â”€â”€ Choose: MICROSERVICES

Team Size?
â”œâ”€â”€ < 10 developers
â”‚   â””â”€â”€ Choose: MONOLITHIC
â”‚
â””â”€â”€ 20+ developers
    â””â”€â”€ Choose: MICROSERVICES
```

---

## ğŸ“ Lessons Learned

### 1. **Context Matters**
- No architecture is universally better
- Choice depends on load, resources, and team size

### 2. **Scaling Changes Everything**
- Monolithic wins at low load
- Microservices wins when scaled under high load

### 3. **The 10ms Tax**
- Microservices has inherent network overhead
- Only worth it when scaling benefits exceed overhead

### 4. **Simplicity Has Value**
- Monolithic is simpler to develop, deploy, and debug
- Choose simplicity unless scale demands complexity

---

## ğŸ“š Resources

### Documentation
- **Standard Test Guide**: `QUICKSTART.md`
- **Scaling Test Guide**: `SCALING_QUICKSTART.md`
- **Presentation Materials**: `PRESENTATION_TAKEAWAYS.md`, `POWERPOINT_SLIDES.md`
- **Monolithic App**: `../tasktracker-mono/README.md`
- **Microservices App**: `../tasktracker-micro/README.md`

### External Resources
- **Locust Documentation**: https://docs.locust.io/
- **FastAPI Documentation**: https://fastapi.tiangolo.com/
- **Docker Compose Scaling**: https://docs.docker.com/compose/

---

## ğŸ‰ Summary

This performance testing suite demonstrates:

âœ… **Complete comparison** under different load conditions  
âœ… **Monolithic wins with limited resources** (2.7x faster)  
âœ… **Microservices wins when scaled under high load** (33% faster, 10% more throughput)  
âœ… **Evidence-based architectural decisions**  
âœ… **Real-world scenarios** with realistic user behavior  
âœ… **Professional-grade testing** with industry-standard tools  

### The Bottom Line:

> **Monolithic is ideal for most applications** (< 100 concurrent users, limited resources)  
>  
> **Microservices shines at scale** (200+ users, horizontal scaling, unlimited resources)  
>  
> **Choose based on your constraints, not the hype!**

---

## ğŸš€ Quick Commands

```bash
# Setup (first time)
./setup.sh

# Standard load test (shows monolithic winning)
./run_both_tests.sh

# High load test with scaling (shows microservices winning)
cd ../tasktracker-micro && ./start-scaled.sh
cd ../tasktracker-performance-tests && ./run_scaled_test.sh

# View results
open results/comparison_*/
open results/monolithic_*/report.html
open results/microservices_*/report.html
```

---

**Perfect for presentations, thesis defense, or architectural decision-making!** ğŸ¯

Happy testing! ğŸš€
