# Performance Testing Project Summary

## âœ… Project Complete

All performance testing infrastructure has been created and configured.

---

## ğŸ“¦ What Was Created

### Core Test Files
- âœ… `locustfile_monolithic.py` - Load tests for monolithic architecture
- âœ… `locustfile_microservices.py` - Load tests for microservices architecture
- âœ… `config.py` - Centralized configuration
- âœ… `utils.py` - Helper functions and test data generation

### Analysis Tools
- âœ… `analyze_results.py` - Generate comparison charts and visualizations

### Run Scripts
- âœ… `setup.sh` - First-time setup (install dependencies)
- âœ… `run_mono_test.sh` - Run monolithic test only
- âœ… `run_micro_test.sh` - Run microservices test only
- âœ… `run_both_tests.sh` - Run both tests in parallel (recommended)
- âœ… `run_tests.sh` - Interactive menu
- âœ… `compare_results.sh` - Compare existing test results
- âœ… `cleanup.sh` - Clean old test results

### Documentation
- âœ… `README.md` - Comprehensive documentation
- âœ… `QUICKSTART.md` - Quick reference guide
- âœ… `requirements.txt` - Python dependencies
- âœ… `.gitignore` - Git ignore rules

---

## ğŸ”§ Configuration Changes

### Monolithic App Ports (Updated)
- **Application**: Port 9000 (was 8000)
- **Database**: Port 5435 (was 5432)
- **README.md**: Updated all references to new ports

This allows both architectures to run simultaneously for parallel testing.

---

## ğŸš€ How to Use

### 1. First Time Setup
```bash
cd tasktracker-performance-tests
./setup.sh
```

### 2. Start Both Applications
```bash
# Terminal 1 - Monolithic (Port 9000)
cd ../tasktracker-mono
docker compose up -d

# Terminal 2 - Microservices (Port 8000)  
cd ../tasktracker-micro
docker compose up -d
```

### 3. Run Tests
```bash
cd tasktracker-performance-tests
source venv/bin/activate

# Interactive menu (easiest)
./run_tests.sh

# Or run both in parallel (recommended)
./run_both_tests.sh

# Or run individually
./run_mono_test.sh
./run_micro_test.sh
```

### 4. View Results
Results are saved in `results/` with timestamps:
- `results/monolithic_*/report.html`
- `results/microservices_*/report.html`
- `results/comparison_*/` (charts and summary)

---

## ğŸ“Š What Gets Tested

### Performance Metrics
- âœ… Response times (average, min, max, percentiles)
- âœ… Throughput (requests per second)
- âœ… Failure rates
- âœ… Concurrent user handling
- âœ… Database performance under load

### API Endpoints Tested
1. **Authentication** (User Service)
   - Login
   - Get user info
   - Token validation

2. **Tasks** (Task Service)
   - Create task
   - List all tasks
   - List filtered tasks (by status, priority)
   - Get single task
   - Update task
   - Mark complete/incomplete
   - Delete task

3. **Statistics** (Stats Service)
   - User statistics
   - Cross-service communication

### Test Characteristics
- **Pre-populated data**: 100 users Ã— 20 tasks = 2000 tasks
- **Realistic patterns**: 70% read, 20% write, 7% update, 3% delete
- **Concurrent users**: Configurable (default 50)
- **Duration**: Configurable (default 60s)
- **Wait time**: 1-3 seconds between operations

---

## ğŸ“ˆ Generated Charts

The analysis script creates 5 comparison charts:

1. **Response Time Comparison** - Avg response by endpoint
2. **Throughput Comparison** - Requests/sec by endpoint
3. **Failure Rate Comparison** - Error rates by endpoint
4. **Percentile Comparison** - Response time distribution
5. **Summary Table** - Overall metrics side-by-side

---

## âš™ï¸ Configuration Options

### Test Parameters
```bash
# Quick test
USERS=30 RUN_TIME=30s ./run_both_tests.sh

# Standard load
USERS=100 RUN_TIME=5m ./run_both_tests.sh

# Stress test
USERS=200 RUN_TIME=10m ./run_both_tests.sh
```

### Edit config.py
- `ARCHITECTURES` - Base URLs
- `TEST_CONFIG` - Default parameters
- `USER_GENERATION` - Pre-test data
- `TASK_WEIGHTS` - Operation distribution

---

## ğŸ¯ Test Scenarios

| Scenario | Command | Purpose |
|----------|---------|---------|
| Quick baseline | `USERS=50 RUN_TIME=60s` | Fast comparison |
| Standard load | `USERS=100 RUN_TIME=5m` | Normal usage |
| Stress test | `USERS=200 RUN_TIME=5m` | High load |
| Endurance | `USERS=100 RUN_TIME=30m` | Stability |

---

## ğŸ” Expected Results

### Monolithic Architecture
- **Lower latency** - No network overhead between services
- **Higher throughput** - Direct database access
- **Better for**: Simple operations, transactions

### Microservices Architecture  
- **Higher latency** - Network calls between services
- **Independent scaling** - Each service scales separately
- **Better for**: Complex systems, fault isolation

---

## ğŸ“ Project Structure

```
tasktracker-performance-tests/
â”œâ”€â”€ Core Tests
â”‚   â”œâ”€â”€ locustfile_monolithic.py
â”‚   â”œâ”€â”€ locustfile_microservices.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ Analysis
â”‚   â””â”€â”€ analyze_results.py
â”‚
â”œâ”€â”€ Scripts (All executable)
â”‚   â”œâ”€â”€ setup.sh
â”‚   â”œâ”€â”€ run_mono_test.sh
â”‚   â”œâ”€â”€ run_micro_test.sh
â”‚   â”œâ”€â”€ run_both_tests.sh
â”‚   â”œâ”€â”€ run_tests.sh
â”‚   â”œâ”€â”€ compare_results.sh
â”‚   â””â”€â”€ cleanup.sh
â”‚
â”œâ”€â”€ Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ QUICKSTART.md
â”‚   â””â”€â”€ PROJECT_SUMMARY.md (this file)
â”‚
â””â”€â”€ Configuration
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Technologies Used

- **Locust** - Load testing framework
- **Python 3.11+** - Test scripts
- **Matplotlib** - Chart generation
- **Pandas** - Data analysis
- **Faker** - Realistic test data
- **Requests** - HTTP client

---

## ğŸ’¡ Tips

1. **Clean state**: Start with fresh databases for accurate results
2. **Warm-up**: Run a short test first
3. **Multiple runs**: Test 3-5 times and average
4. **Monitor resources**: Watch CPU, memory, database connections
5. **Compare fairly**: Same hardware, same conditions

---

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| Apps not running | Start with `docker compose up -d` |
| Port conflicts | Check ports: 8000 (micro), 9000 (mono) |
| Many failures | Reduce concurrent users |
| Slow setup | Reduce `USER_GENERATION` in config.py |
| Permission denied | Run `chmod +x *.sh` |

---

## ğŸ“š Further Reading

- **Locust Docs**: https://docs.locust.io/
- **Mono App**: `../tasktracker-mono/README.md`
- **Micro App**: `../tasktracker-micro/README.md`
- **Architecture Comparison**: `../tasktracker-micro/ARCHITECTURE_COMPARISON.md`

---

## âœ¨ Key Features

âœ… Comprehensive E2E testing  
âœ… Parallel test execution  
âœ… Automatic data generation  
âœ… Visual comparison charts  
âœ… HTML interactive reports  
âœ… Configurable parameters  
âœ… Easy-to-use scripts  
âœ… Detailed documentation  

---

## ğŸ‰ Ready to Test!

Everything is set up and ready to go. Just run:

```bash
./setup.sh
./run_both_tests.sh
```

Then open the generated HTML reports and comparison charts!

Happy testing! ğŸš€

